{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "## Healthcare Chatbot - LLM Fine-tuning with LoRA\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/YOUR_USERNAME/YOUR_REPO/blob/main/healthcare_chatbot_finetuning.ipynb)\n",
    "\n",
    "### ðŸ“– Project Overview\n",
    "\n",
    "This notebook implements a **domain-specific healthcare assistant** by fine-tuning a Large Language Model using **LoRA (Low-Rank Adaptation)**. The chatbot provides accurate medical information while maintaining ethical guardrails.\n",
    "\n",
    "### Key Features:\n",
    "-  Medical domain specialization\n",
    "-  Parameter-efficient fine-tuning (LoRA)\n",
    "-  Comprehensive evaluation (BLEU, ROUGE)\n",
    "-  Interactive Gradio interface\n",
    "-  Medical safety disclaimers\n",
    "\n",
    "### Pipeline:\n",
    "1. **Setup** - Install dependencies and configure environment\n",
    "2. **Data Preprocessing** - Load and format medical Q&A dataset\n",
    "3. **Model Loading** - Load base model with 4-bit quantization\n",
    "4. **Fine-tuning** - Train with LoRA on medical data\n",
    "5. **Evaluation** - Calculate metrics and compare models\n",
    "6. **Deployment** - Create interactive web interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "# 1. Environment Setup\n",
    "\n",
    "Installing all required packages and verifying GPU availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 78078,
     "status": "ok",
     "timestamp": 1770849936860,
     "user": {
      "displayName": "Limpho Semakale",
      "userId": "11698043166095373885"
     },
     "user_tz": -120
    },
    "id": "install_packages"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install required packages (suppress output for cleaner notebook)\n",
    "!pip install -q transformers>=4.35.0\n",
    "!pip install -q datasets>=2.14.0\n",
    "!pip install -q accelerate>=0.24.0\n",
    "!pip install -q peft>=0.6.0\n",
    "!pip install -q bitsandbytes>=0.41.0\n",
    "!pip install -q trl>=0.7.0\n",
    "!pip install -q gradio>=4.0.0\n",
    "!pip install -q evaluate>=0.4.0\n",
    "!pip install -q rouge-score>=0.1.2\n",
    "!pip install -q nltk>=3.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 55500,
     "status": "ok",
     "timestamp": 1770849997063,
     "user": {
      "displayName": "Limpho Semakale",
      "userId": "11698043166095373885"
     },
     "user_tz": -120
    },
    "id": "imports",
    "outputId": "cb24dc5b-f1c6-4eab-c2e7-eff7f5808070"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Hugging Face\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "# PEFT and LoRA\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Evaluation\n",
    "from evaluate import load as load_metric\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Gradio for deployment\n",
    "import gradio as gr\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1770850005264,
     "user": {
      "displayName": "Limpho Semakale",
      "userId": "11698043166095373885"
     },
     "user_tz": -120
    },
    "id": "check_gpu",
    "outputId": "5ce30ca9-ccbf-44af-fb23-7844675d3931"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability and specs\n",
    "print(\"=\"*80)\n",
    "print(\"GPU INFORMATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ… GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"âŒ No GPU available. Training will be slow on CPU.\")\n",
    "    print(\"   Please enable GPU in Runtime > Change runtime type > T4 GPU\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config"
   },
   "source": [
    "# 2. Configuration\n",
    "\n",
    "Set all hyperparameters and configurations in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1770850150749,
     "user": {
      "displayName": "Limpho Semakale",
      "userId": "11698043166095373885"
     },
     "user_tz": -120
    },
    "id": "config_params",
    "outputId": "a7532a3d-c945-44e6-c7d6-f661e4626878"
   },
   "outputs": [],
   "source": [
    "# CONFIGURATION PARAMETERS\n",
    "\n",
    "# Dataset Configuration\n",
    "DATASET_NAME = \"medalpaca/medical_meadow_medical_flashcards\"\n",
    "TRAIN_SIZE = 3000\n",
    "VAL_SIZE = 500\n",
    "TEST_SIZE = 500\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Model Configuration\n",
    "MODEL_NAME = \"google/gemma-2b\"\n",
    "USE_4BIT_QUANTIZATION = True\n",
    "\n",
    "# LoRA Configuration\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "LORA_TARGET_MODULES = [\"q_proj\", \"v_proj\"]\n",
    "\n",
    "# Training Configuration\n",
    "NUM_EPOCHS = 2\n",
    "BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "MAX_SEQ_LENGTH = 512\n",
    "WARMUP_STEPS = 50\n",
    "\n",
    "# Evaluation Configuration\n",
    "EVAL_SAMPLES = 100\n",
    "CALCULATE_PERPLEXITY = False\n",
    "\n",
    "# Output Configuration\n",
    "OUTPUT_DIR = \"./healthcare-chatbot-lora\"\n",
    "SAVE_STEPS = 100\n",
    "LOGGING_STEPS = 10\n",
    "EVAL_STEPS = 50\n",
    "\n",
    "# Display configuration\n",
    "print(\"=\"*80)\n",
    "print(\"EXPERIMENT CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Dataset: {DATASET_NAME}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Training samples: {TRAIN_SIZE}\")\n",
    "print(f\"LoRA rank: {LORA_R}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_section"
   },
   "source": [
    "# 3. Data Preprocessing\n",
    "\n",
    "Load and prepare the medical Q&A dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411,
     "referenced_widgets": [
      "cfcfe27709c8434dbb7f20f2672efdee",
      "65769e94a1974b7ea20201a355d51ebe",
      "c271ba3bd4c54fe4bed667de8720fe26",
      "d1568f057b3f4976af4999c429ec176e",
      "1d761b078a964d87bbe52ccce447e827",
      "01670c027d8b4ceb96d31aba6d0a0dfa",
      "c5ff07d953e143dd8beca614f4b15143",
      "40ff94def876402a819dc1eac1a41085",
      "b89c54fed7e742569f91521473c98403",
      "ac39c103c9624cedad6a66d5909067c3",
      "b6adab239e0a4798af5bd06547266f1c",
      "fe89ca917e1e4e3398d165e485f8d631",
      "6b19eece91b54a9780fd9d62e6f3049c",
      "e6b886a30b7f4a2bbc3557028dacde99",
      "a15edc2e7dcd4453ac85925198943d47",
      "0a7d20770eeb4e3e9e1621f38b7b771a",
      "9666e2e4c2f2490f9445aab2400b59d3",
      "cc609549837744c4be8dfb872ab419f0",
      "08e5279a68a44ee6ab3b3b1bc16e66bd",
      "3ac37ad375554ef8b026b2f369d6b0b7",
      "9efd6692266746c4854dd5323af4e852",
      "aadb1ffeee2546e5a94335221c079f0f",
      "0b152feb9d0a42929d70934667fb84b9",
      "97f9b7a59e8941abb06861478e1fddb8",
      "3a3ccec0e0e243e9a504947bbf1f2646",
      "5bd5b1bb75334c05b554ea94667fefd6",
      "589d7aba149e4701be96ab4495e84df8",
      "1fa2a6437146457bbb8dd73792160e7d",
      "d1fd5d0e933f479ab84a683ba76fd514",
      "ad5d142d65264eceb3c311295ce9fb07",
      "ff1bf332bab546dfb2dd83e1cd7054ad",
      "e15071c1863f4e7bba5170a760b077ef",
      "b4a07fad15b043d3aca1d60a6e7f2937"
     ]
    },
    "executionInfo": {
     "elapsed": 7381,
     "status": "ok",
     "timestamp": 1770850170539,
     "user": {
      "displayName": "Limpho Semakale",
      "userId": "11698043166095373885"
     },
     "user_tz": -120
    },
    "id": "load_dataset",
    "outputId": "c2681eff-81ee-4cdc-c1c6-e1ad0ed1d327"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "DATASET_NAME = \"medalpaca/medical_meadow_medical_flashcards\"\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset(DATASET_NAME)\n",
    "\n",
    "print(f\"\\nâœ… Dataset loaded successfully!\")\n",
    "print(f\"   Total samples: {len(dataset['train'])}\")\n",
    "print(f\"   Dataset structure: {dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1770850206175,
     "user": {
      "displayName": "Limpho Semakale",
      "userId": "11698043166095373885"
     },
     "user_tz": -120
    },
    "id": "explore_data",
    "outputId": "a11f2fe3-d5b2-41e4-8845-1f197f1a0854"
   },
   "outputs": [],
   "source": [
    "# Explore sample data\n",
    "print(\"=\"*80)\n",
    "print(\"SAMPLE DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in range(3):\n",
    "    sample = dataset['train'][i]\n",
    "    print(f\"\\n--- Sample {i+1} ---\")\n",
    "\n",
    "    # Extract fields\n",
    "    question = sample.get('instruction', sample.get('input', sample.get('question', '')))\n",
    "    answer = sample.get('output', sample.get('answer', sample.get('response', '')))\n",
    "\n",
    "    print(f\"Question: {question[:150]}...\")\n",
    "    print(f\"Answer: {answer[:150]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1770850229895,
     "user": {
      "displayName": "Limpho Semakale",
      "userId": "11698043166095373885"
     },
     "user_tz": -120
    },
    "id": "format_function",
    "outputId": "38d8e3b5-ca2b-42ed-c173-d29eb050b8b1"
   },
   "outputs": [],
   "source": [
    "# Define formatting function\n",
    "def format_instruction(sample):\n",
    "    \"\"\"\n",
    "    Format data into instruction-following template.\n",
    "    \"\"\"\n",
    "    # Extract fields\n",
    "    instruction = sample.get('instruction', sample.get('input', sample.get('question', '')))\n",
    "    response = sample.get('output', sample.get('answer', sample.get('response', '')))\n",
    "\n",
    "    # Create prompt\n",
    "    prompt = f\"\"\"Below is a medical question. Provide an accurate, helpful, and professional response.\n",
    "\n",
    "### Question:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "{response}\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Test formatting\n",
    "print(\"Formatted Example:\")\n",
    "print(\"=\"*80)\n",
    "print(format_instruction(dataset['train'][0]))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217,
     "referenced_widgets": [
      "6df926c6151a4354880a5c118eda153a",
      "54e87d067d614b4d9f56a9fffeb848b7",
      "76c4df85f3704d479f2efea720765f33",
      "d7384df6c67e4569bf1554e090b169b6",
      "c0e7d53a28ef41c48c55370754f2bbf7",
      "8f2e60d1316140ee82ab200edbebce3a",
      "c9ba34659a4e47ad97f59411e98387a7",
      "4017fb3504934d789b0b0d81228f971b",
      "1d5941c9014e400db9f9879eb19e62bc",
      "eac9af8ab1df4582953ff10217c13819",
      "c95f038bcf324e60b444dfe905778803",
      "f3d01a15a68849b69bb342ff8592d5ea",
      "b626bb651b6b40c0a73fb644b97cfaee",
      "3ca191bb87b642089bf267f3b4e83962",
      "bed92cae69cb4595a05988abd755098d",
      "cf6f3079f2094288acc1b500454ce956",
      "e30c4817643c40cfa5895ddd7c3577d5",
      "603c3c215e9e46ba82b2fdc7aa8ce835",
      "a71d6ef7e66d4072ac0051a07cda87bc",
      "78fb80f93ae040fe9f441abaa481b463",
      "cd08cfa773824994a6ad21e5a3b0bb5f",
      "bb9fcd0bf1f947f2944961c11cdf0105",
      "2109db1166be419ebd3fd6579371bbbf",
      "6c93e8fb0a964eeaa5552434d1ac8470",
      "8e4a11f1253d477e8c94bafff88ee237",
      "66993ecd9bc54dc58fa8a224307c0140",
      "2d7010e208e24f259be821970a567d31",
      "ec8b29ffbf4640e08ca2aa793fd9ec3d",
      "200a7e0186124833990eb72aee0a2220",
      "07d10e73b9b94c60ac931db27c17a298",
      "0232acc887c3439ea66eac05f81dcccb",
      "52282d48e20e4115af2d2c16de9aa6ff",
      "532522e7bd224f5fabea7ff8268f88af"
     ]
    },
    "executionInfo": {
     "elapsed": 1039,
     "status": "ok",
     "timestamp": 1770850250768,
     "user": {
      "displayName": "Limpho Semakale",
      "userId": "11698043166095373885"
     },
     "user_tz": -120
    },
    "id": "create_splits",
    "outputId": "70296c3b-5542-4146-f468-35e5d0486678"
   },
   "outputs": [],
   "source": [
    "# Create train/validation/test splits\n",
    "print(\"Creating dataset splits...\")\n",
    "\n",
    "# Shuffle dataset\n",
    "dataset_shuffled = dataset['train'].shuffle(seed=RANDOM_SEED)\n",
    "\n",
    "# Create splits\n",
    "train_dataset = dataset_shuffled.select(range(TRAIN_SIZE))\n",
    "val_dataset = dataset_shuffled.select(range(TRAIN_SIZE, TRAIN_SIZE + VAL_SIZE))\n",
    "test_dataset = dataset_shuffled.select(range(TRAIN_SIZE + VAL_SIZE,\n",
    "                                             TRAIN_SIZE + VAL_SIZE + TEST_SIZE))\n",
    "\n",
    "# Add formatted text field\n",
    "def add_text_field(example):\n",
    "    example['text'] = format_instruction(example)\n",
    "    return example\n",
    "\n",
    "train_dataset = train_dataset.map(add_text_field)\n",
    "val_dataset = val_dataset.map(add_text_field)\n",
    "test_dataset = test_dataset.map(add_text_field)\n",
    "\n",
    "print(f\"\\n Splits created:\")\n",
    "print(f\"   Training: {len(train_dataset)} samples\")\n",
    "print(f\"   Validation: {len(val_dataset)} samples\")\n",
    "print(f\"   Test: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 611
    },
    "executionInfo": {
     "elapsed": 630,
     "status": "ok",
     "timestamp": 1770850257673,
     "user": {
      "displayName": "Limpho Semakale",
      "userId": "11698043166095373885"
     },
     "user_tz": -120
    },
    "id": "data_stats",
    "outputId": "422393ca-33cf-4462-b805-e9cdcbc93d1b"
   },
   "outputs": [],
   "source": [
    "# Calculate dataset statistics\n",
    "lengths = [len(sample['text']) for sample in train_dataset]\n",
    "\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(f\"  Average length: {np.mean(lengths):.1f} characters\")\n",
    "print(f\"  Median length: {np.median(lengths):.1f} characters\")\n",
    "print(f\"  Min length: {np.min(lengths)} characters\")\n",
    "print(f\"  Max length: {np.max(lengths)} characters\")\n",
    "\n",
    "# Visualize length distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(lengths, bins=50, color='skyblue', edgecolor='black')\n",
    "plt.axvline(np.mean(lengths), color='red', linestyle='--', label=f'Mean: {np.mean(lengths):.1f}')\n",
    "plt.xlabel('Text Length (characters)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Training Sample Lengths')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_section"
   },
   "source": [
    "# 4. Model Loading and Configuration\n",
    "\n",
    "Load the base model with 4-bit quantization and prepare for LoRA fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "fec8eac166134effba54cfa9f586d093",
      "a18abe0dfc914edabf5915c20a244fe9",
      "fbff916a34f34bb68c33d3f09f5b42ed",
      "05622690ba1843d084e89d456ac72cbc",
      "7707c0e19b904416a2e320628edb2ab8",
      "43733d5dc9cd40dbb6cdbd8d7990df54",
      "7d42d76bdb2942e3aa8204a1a533e326",
      "52e74889284646ef957d05cea853d32d",
      "73ead5d0a56c47629fe2e018b217a3d4",
      "c63498ea08ed44679a91ad0fbc65d912",
      "4a0f7a87fb504d90b100483cce07c440",
      "c401204c4d6a413c98412a6a03bd1ad3",
      "c4a73d1db1da4856b81d60368f6d7dfd",
      "36c1586893ef41e0a1d58bbc6a0947f5",
      "f3f62126b1fa4810bc648ff3e5bf0245",
      "ebf5a6a3b3aa447386453337a24540d6",
      "fee06da30bc34021a261953aad52e17b",
      "6b9cc796695d439d8749b129a532e3f8",
      "666798141a10479eb220dbe7c538863a",
      "3af0e7d500744f4c8ae9f83d47fcb9d6"
     ]
    },
    "executionInfo": {
     "elapsed": 77,
     "status": "ok",
     "timestamp": 1770850279000,
     "user": {
      "displayName": "Limpho Semakale",
      "userId": "11698043166095373885"
     },
     "user_tz": -120
    },
    "id": "YH4iPwpQK-dO",
    "outputId": "bc810cb9-875e-4a27-9c57-182f43304d21"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180,
     "referenced_widgets": [
      "0984e2c6286f496c92040d94c4e50b82",
      "2a8bcfcc3e30455c8d88eae323c9190e",
      "f82ff9f932f343d6b6e28d63879e8761",
      "3986807d39764f81982ff08b0b8a0222",
      "b20f74d378a1468498d8dcad155f35e7",
      "ef3b796a66c44131a622bfd27daacbb8",
      "e6304d9076c947188eec1a02ac61a82b",
      "2480b494c86d446c8d19a9fa81d281cd",
      "c0a6e79e98504668a4e92bf2c5028d12",
      "c97637804590499aa2f8d56e65350520",
      "e99c9d44723b4169a58541cb6dc96dd0",
      "132b9ac041af40f5948c851b6dc96c69",
      "744c337b875e457d8c4c27ec465681eb",
      "b40f8f4ed50148d68b2665e26c294f7d",
      "b6bb8bafbd7e4fbda478a9d823aaeb5f",
      "71aa6d0c231d4d60b5f20c4de49c5685",
      "0a072ef0bd3e4b75afcf51a4e2a6c3d6",
      "139d217dc92d4282840b6312dd02b48d",
      "78230d1f630041b1a611eab7ed71ef8e",
      "7fe1c417cab64de79b4405837197edda",
      "e6abb56eac9a41ed9f0f82a55e7c3057",
      "01138709ebe14c2f99bd8bffb1d47795",
      "77825981e74e4170a4bcc8a71b33cced",
      "869d7645c3d94f18a705af4ec76c98bd",
      "acb2110258004d8590d2ebbcefb7ed06",
      "f58bfdc1a39a496a9f8d6ff26372507e",
      "39159ebe2ced465195fd093dbbeda4ce",
      "d836a1e558b44d39bdfc01462aa63239",
      "d87095dc8ee9446aaed4efa877bf530b",
      "50a6a3a0e38349a0b8a045efd6581f2c",
      "f2f151141b984b9290e341a65a3b9ad0",
      "053824a7d4f9447b9dee33cbb04a2e9c",
      "d5bcaa31027b47e285e610a0555443e9",
      "59814623dd6b4c2bb5145c4c7f554b45",
      "18b45a52ad0e4bff873ea0bc08652abc",
      "5ddb888b5ca24a1a882e0e7f27f6d429",
      "4135022fe74e4d3db3bb26a19d62afed",
      "bdde203319c64ee59fbc94709f9bbad3",
      "9748fd29e299428ebd1567169c41ef15",
      "a458dd4aea28491dbc182dd9f7a858ac",
      "93a0b072cf62426591b4665041e6404b",
      "f233bac0984944f79976aa854b8b4ba4",
      "318b5a15ffdc47c5a92e0ce8762718a8",
      "34a06ddfdc034d63a5ef6b6964a8166d"
     ]
    },
    "executionInfo": {
     "elapsed": 10442,
     "status": "ok",
     "timestamp": 1770850327418,
     "user": {
      "displayName": "Limpho Semakale",
      "userId": "11698043166095373885"
     },
     "user_tz": -120
    },
    "id": "load_tokenizer",
    "outputId": "af4ef3a3-733c-4a38-8a8a-7fff7fbbc8a7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"google/gemma-2b\"\n",
    "\n",
    "print(f\"Loading tokenizer: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    token=os.environ.get(\"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\")\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"âœ… Tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153,
     "referenced_widgets": [
      "60da5e63a76f4945a1a5e7711f0aef62",
      "6be0e67ff34344a1b0d21867dcab4a0d",
      "e45567653009462f9be7c937aa3cc7e7",
      "aed1a2c130b74882a05cbe579a2c68d6",
      "4c6d8ac532874ab5ba787b9a3e3a1e49",
      "244a1fd63e674ad28ec836522641c591",
      "598509ce082a4fb1badf05b8be668fa7",
      "64322da2f0a24683ab8ffc65315777b9",
      "1e94ec41276e4d7cb70e8dfa007421e4",
      "1cf72c0a1a60435c911a894566690b24",
      "db37798b9eb34b86b260b162694e8d22"
     ]
    },
    "executionInfo": {
     "elapsed": 16119,
     "status": "ok",
     "timestamp": 1770850528089,
     "user": {
      "displayName": "Limpho Semakale",
      "userId": "11698043166095373885"
     },
     "user_tz": -120
    },
    "id": "load_model",
    "outputId": "30c0012f-1811-44f8-d8d9-04bf14275317"
   },
   "outputs": [],
   "source": [
    "# Configure 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load base model\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(f\"\\n Model loaded successfully!\")\n",
    "print(f\"   Total parameters: {base_model.num_parameters() / 1e6:.2f}M\")\n",
    "print(f\"   Memory footprint: {base_model.get_memory_footprint() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6165,
     "status": "ok",
     "timestamp": 1770850555933,
     "user": {
      "displayName": "Limpho Semakale",
      "userId": "11698043166095373885"
     },
     "user_tz": -120
    },
    "id": "test_base_model",
    "outputId": "758ce233-426f-4e5b-b7c9-27d781584614"
   },
   "outputs": [],
   "source": [
    "# Test base model (before fine-tuning)\n",
    "def generate_response(model, tokenizer, prompt, max_new_tokens=200):\n",
    "    \"\"\"Generate response from model.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test prompt\n",
    "test_prompt = \"\"\"Below is a medical question. Provide an accurate response.\n",
    "\n",
    "### Question:\n",
    "What are the symptoms of diabetes?\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TESTING BASE MODEL (Before Fine-tuning)\")\n",
    "print(\"=\"*80)\n",
    "base_response = generate_response(base_model, tokenizer, test_prompt)\n",
    "print(base_response)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lora_section"
   },
   "source": [
    "# 5. LoRA Configuration and Model Preparation\n",
    "\n",
    "Apply LoRA for parameter-efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1770850565956,
     "user": {
      "displayName": "Limpho Semakale",
      "userId": "11698043166095373885"
     },
     "user_tz": -120
    },
    "id": "configure_lora",
    "outputId": "ccdb39dc-e867-4e49-d0c7-1b389b5aae48"
   },
   "outputs": [],
   "source": [
    "# Prepare model for training\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "# Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=LORA_TARGET_MODULES\n",
    ")\n",
    "\n",
    "print(\"LoRA Configuration:\")\n",
    "print(f\"  Rank (r): {LORA_R}\")\n",
    "print(f\"  Alpha: {LORA_ALPHA}\")\n",
    "print(f\"  Dropout: {LORA_DROPOUT}\")\n",
    "print(f\"  Target modules: {LORA_TARGET_MODULES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 76,
     "status": "ok",
     "timestamp": 1770850588348,
     "user": {
      "displayName": "Limpho Semakale",
      "userId": "11698043166095373885"
     },
     "user_tz": -120
    },
    "id": "apply_lora",
    "outputId": "a543f9cb-d8f7-40aa-c76d-e6b59cea9377"
   },
   "outputs": [],
   "source": [
    "# Apply LoRA to model\n",
    "model = get_peft_model(base_model, peft_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "model.print_trainable_parameters()\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training_section"
   },
   "source": [
    "# 6. Model Training\n",
    "\n",
    "Fine-tune the model using the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 93,
     "status": "ok",
     "timestamp": 1770850604551,
     "user": {
      "displayName": "Limpho Semakale",
      "userId": "11698043166095373885"
     },
     "user_tz": -120
    },
    "id": "training_args",
    "outputId": "83dcd8ba-7b18-423a-ed6a-07c072a85568"
   },
   "outputs": [],
   "source": [
    "# Configure training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    fp16=False, # Changed to False to avoid BFloat16 NotImplementedError\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=False, # Set to False as evaluation_strategy is not recognized\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=\"none\",\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Batch size per device: {BATCH_SIZE}\")\n",
    "print(f\"  Gradient accumulation: {GRADIENT_ACCUMULATION}\")\n",
    "print(f\"  Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Total training steps: ~{(len(train_dataset) // (BATCH_SIZE * GRADIENT_ACCUMULATION)) * NUM_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226,
     "referenced_widgets": [
      "c013ddd398044513b20a746c9dee2f25",
      "c0c4f10c5e91453eb22f9ed344c06640",
      "60ea2e227b7e47e2a26705508179c158",
      "a71b0f294d274ba7aa73b9280dbdbb55",
      "36cd826283ab4ffc82fb4bc87310bab6",
      "b06117e65cbe4475ba89f7a0e168aeff",
      "e5d5c3a839a7451fbdc0b3e85d7c5727",
      "489d505639e34d05b0cbad03b2222949",
      "fb9a5ad5bc214583922fe35db7b2961d",
      "6fc2d17d04554d76829ad1c9df4eac03",
      "c42a3445cec940e6933c6d09c940fbb4",
      "a67ea2b941af4d82bbc38eea522cf1db",
      "f21a6e0293bb4dd5b702a68adbc70745",
      "792e8c10ddf84b5c9410ee47059b1022",
      "287204d00c6a4df4b59407330eef17a6",
      "ff2726d05c5a4cbba6dbcd353adb1137",
      "3c89c06e479345c0bae33747d2cab7d2",
      "173de25583e74c779532bd0d00883ef6",
      "220fde94ee8640a9bd2ea5f08bbaf841",
      "54ad17d2f8784723ba7b6904d12d6d9e",
      "7db3e30e0349457abacc4410a109e1d6",
      "a3c9ffebafc04d838f50b0e8b7d8ea3d",
      "325df1c2e30f4a93ad3507be2a924fe0",
      "5f2bb335aacb406e9d2b22ba8778d9ac",
      "93e6403dd194427b89903ca2abbf6105",
      "15b1f4ba42074f128d0353f34e035e5d",
      "610c425436e8410086930c266071ee6d",
      "7c6316e2207140c0a788fc4452b90b1e",
      "4c40403a4f8d43f492e5f62278750a1d",
      "6fe79e0d7e8a42d8965ebd1a0ec2fa21",
      "dc923d23e3c94d05b432ded15ba6b7f1",
      "ee2b66b07966457fb5b6948999c0302f",
      "e29540ea74674f808569f65eea641aa7",
      "ff6cbd1e0ebe4df883f96d50340d4744",
      "5231196175054b63a9c6d4001c239276",
      "93e36247fff149c78b05d5bc52093ec0",
      "9347bb6dde9d46d1b0fc6977f69d4d5b",
      "a66b1c8c42ec4f80bdb48454c957cd7f",
      "7ee92286afab4baaa0d1d2d8d8a9546a",
      "6274ca2640564bb980516626444f52ee",
      "5bc67bbaf9cb413ab41cd58c9b618c21",
      "291db4a430984996af2ebc27d1486d30",
      "e13ca1d00c3c418595d8a5db742c46d3",
      "25e0371add9d4d75bcfec66d0aab797b",
      "b1ff3ceb4fdb4c80b15bcc5d95685696",
      "0113d5bcd22d4441af02cd4ea52148a6",
      "ee9c794a85cb4730975d4168f41bc596",
      "cdb5db1544bf4932929533575893b493",
      "7af91b053db248a48ea23291e29bd9a3",
      "19bf3598233944d4a16143c2ed6bdf19",
      "d4ad1bbecc5a4859a850072e67fbe764",
      "ee633dd7aa9f4f2d87e4652a26f0a470",
      "4eb1d1fb9e0140dc9d2323895c852498",
      "249fbca029524316a595d57919327824",
      "d9757314b1364bc1a6131a6efd3b3a7c",
      "2ba0ac49d8724a248d4c28f90a188e7e",
      "46f12282b9b24330b8e46def4ad4903b",
      "d02daf9432b645a69162300457c32c32",
      "cf545b052701441ca4b1fb96d165b8be",
      "983d9995d49e459e868f41420c135d12",
      "271ed059adbe44cbb11ff80473367e08",
      "b0641c414f6e4e32b104465b730337c5",
      "d55aa39acbc9452bb95d6c43bacafc81",
      "a0935c65dc52415ca9e5b9990778cec5",
      "330c2b76562e4461862ee9a005c9e1fe",
      "eb697f9f4c244b2d9b91beda17bf54b3"
     ]
    },
    "executionInfo": {
     "elapsed": 16778,
     "status": "ok",
     "timestamp": 1770851262552,
     "user": {
      "displayName": "Limpho Semakale",
      "userId": "11698043166095373885"
     },
     "user_tz": -120
    },
    "id": "initialize_trainer",
    "outputId": "c0f44d9e-901a-46b3-ae0f-3a3181751c06"
   },
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "print(\" Trainer initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1961231,
     "status": "ok",
     "timestamp": 1770853232390,
     "user": {
      "displayName": "Limpho Semakale",
      "userId": "11698043166095373885"
     },
     "user_tz": -120
    },
    "id": "train_model",
    "outputId": "ab3f8014-90cc-43b0-e6a8-fe12b412a079"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*80)\n",
    "print(\"This will take approximately 30-60 minutes depending on your GPU.\")\n",
    "print(\"You can monitor progress below...\\n\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Train\n",
    "train_result = trainer.train()\n",
    "\n",
    "# Calculate duration\n",
    "end_time = datetime.now()\n",
    "training_duration = (end_time - start_time).total_seconds() / 60\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Training duration: {training_duration:.2f} minutes\")\n",
    "print(f\"Final training loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"Total steps: {train_result.global_step}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 979,
     "status": "ok",
     "timestamp": 1770853240502,
     "user": {
      "displayName": "Limpho Semakale",
      "userId": "11698043166095373885"
     },
     "user_tz": -120
    },
    "id": "save_model",
    "outputId": "99bd6504-1344-4e8a-8f57-eff4a33d7437"
   },
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "final_model_dir = os.path.join(OUTPUT_DIR, \"final\")\n",
    "\n",
    "trainer.model.save_pretrained(final_model_dir)\n",
    "tokenizer.save_pretrained(final_model_dir)\n",
    "\n",
    "print(f\" Model saved to: {final_model_dir}\")\n",
    "\n",
    "# Save training info\n",
    "training_info = {\n",
    "    'model': MODEL_NAME,\n",
    "    'dataset': DATASET_NAME,\n",
    "    'train_samples': TRAIN_SIZE,\n",
    "    'val_samples': VAL_SIZE,\n",
    "    'lora_r': LORA_R,\n",
    "    'lora_alpha': LORA_ALPHA,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'epochs': NUM_EPOCHS,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'training_duration_minutes': training_duration,\n",
    "    'final_loss': float(train_result.training_loss)\n",
    "}\n",
    "\n",
    "with open(os.path.join(final_model_dir, 'training_info.json'), 'w') as f:\n",
    "    json.dump(training_info, f, indent=2)\n",
    "\n",
    "print(\" Training info saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation_section"
   },
   "source": [
    "# 7. Model Evaluation\n",
    "\n",
    "Evaluate the fine-tuned model using BLEU and ROUGE metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "6bc059fe19144c9a91a0f6bc256c4863",
      "ff39333ef6dc4c5f9e986fed205ed650",
      "b72cdd7e262f4f1b9fdb7a99f666398a",
      "0371216f192543a58a9f125b0379fb27",
      "bf7e267229ae44b099b2dd44c7b63838",
      "5c91a8058611414faa1942654c340206",
      "bb5ee2d59e4d408cb7e504082908c830",
      "afcba207a9564da08da58d8203dc4f74",
      "8a32e0cd57094e2f97f1f01eda6fbc0a",
      "f0d9a02ea60545bdb44617ca8d66167c",
      "b2001f97d6ad4f99b14239599f725890"
     ]
    },
    "executionInfo": {
     "elapsed": 1980,
     "status": "ok",
     "timestamp": 1770853279882,
     "user": {
      "displayName": "Limpho Semakale",
      "userId": "11698043166095373885"
     },
     "user_tz": -120
    },
    "id": "load_metrics",
    "outputId": "2703e578-476a-4164-a52e-bc3a1a50b013"
   },
   "outputs": [],
   "source": [
    "# Load evaluation metrics\n",
    "rouge_metric = load_metric('rouge')\n",
    "smoother = SmoothingFunction()\n",
    "\n",
    "print(\" Evaluation metrics loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1057186,
     "status": "ok",
     "timestamp": 1770854342519,
     "user": {
      "displayName": "Limpho Semakale",
      "userId": "11698043166095373885"
     },
     "user_tz": -120
    },
    "id": "generate_predictions",
    "outputId": "7735a3d9-98ba-4b26-fb56-54bd6bd4e408"
   },
   "outputs": [],
   "source": [
    "# Generate predictions on test set\n",
    "print(f\"Generating predictions on {EVAL_SAMPLES} test samples...\")\n",
    "print(\"This may take 5-10 minutes...\\n\")\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for i in range(min(EVAL_SAMPLES, len(test_dataset))):\n",
    "    if i % 20 == 0:\n",
    "        print(f\"Progress: {i}/{EVAL_SAMPLES}\")\n",
    "\n",
    "    sample = test_dataset[i]\n",
    "    question = sample.get('instruction', sample.get('input', sample.get('question', '')))\n",
    "    reference = sample.get('output', sample.get('answer', sample.get('response', '')))\n",
    "\n",
    "    # Create prompt\n",
    "    prompt = f\"\"\"Below is a medical question. Provide an accurate response.\n",
    "\n",
    "### Question:\n",
    "{question}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "    # Generate prediction\n",
    "    pred = generate_response(model, tokenizer, prompt, max_new_tokens=150)\n",
    "\n",
    "    # Extract response part\n",
    "    if \"### Response:\" in pred:\n",
    "        pred = pred.split(\"### Response:\")[1].strip()\n",
    "\n",
    "    predictions.append(pred)\n",
    "    references.append(reference)\n",
    "\n",
    "print(f\"\\n Generated {len(predictions)} predictions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 60,
     "status": "ok",
     "timestamp": 1770854373360,
     "user": {
      "displayName": "Limpho Semakale",
      "userId": "11698043166095373885"
     },
     "user_tz": -120
    },
    "id": "calculate_bleu",
    "outputId": "b3f1b467-cf5d-4424-ef00-fdd35d1b216c"
   },
   "outputs": [],
   "source": [
    "# Calculate BLEU scores\n",
    "print(\"Calculating BLEU scores...\")\n",
    "\n",
    "bleu_scores = []\n",
    "for pred, ref in zip(predictions, references):\n",
    "    pred_tokens = pred.split()\n",
    "    ref_tokens = [ref.split()]\n",
    "    bleu = sentence_bleu(ref_tokens, pred_tokens, smoothing_function=smoother.method1)\n",
    "    bleu_scores.append(bleu)\n",
    "\n",
    "avg_bleu = np.mean(bleu_scores)\n",
    "print(f\" Average BLEU Score: {avg_bleu:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 199,
     "status": "ok",
     "timestamp": 1770854388348,
     "user": {
      "displayName": "Limpho Semakale",
      "userId": "11698043166095373885"
     },
     "user_tz": -120
    },
    "id": "calculate_rouge",
    "outputId": "17e61ffa-1d1e-468b-b333-7bf589063b8c"
   },
   "outputs": [],
   "source": [
    "# Calculate ROUGE scores\n",
    "print(\"Calculating ROUGE scores...\")\n",
    "\n",
    "rouge_results = rouge_metric.compute(\n",
    "    predictions=predictions,\n",
    "    references=references,\n",
    "    use_aggregator=True\n",
    ")\n",
    "\n",
    "print(\" ROUGE Scores calculated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1770854391981,
     "user": {
      "displayName": "Limpho Semakale",
      "userId": "11698043166095373885"
     },
     "user_tz": -120
    },
    "id": "display_results",
    "outputId": "8ac8a31e-0867-4794-e25e-51f050a31c1a"
   },
   "outputs": [],
   "source": [
    "# Display evaluation results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nBLEU Score:    {avg_bleu:.4f}\")\n",
    "print(f\"ROUGE-1:       {rouge_results['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2:       {rouge_results['rouge2']:.4f}\")\n",
    "print(f\"ROUGE-L:       {rouge_results['rougeL']:.4f}\")\n",
    "print(f\"\\nSamples evaluated: {len(predictions)}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save results\n",
    "eval_results = {\n",
    "    'bleu': float(avg_bleu),\n",
    "    'rouge1': float(rouge_results['rouge1']),\n",
    "    'rouge2': float(rouge_results['rouge2']),\n",
    "    'rougeL': float(rouge_results['rougeL']),\n",
    "    'num_samples': len(predictions)\n",
    "}\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, 'evaluation_results.json'), 'w') as f:\n",
    "    json.dump(eval_results, f, indent=2)\n",
    "\n",
    "print(\"\\n Results saved to evaluation_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 63357,
     "status": "ok",
     "timestamp": 1770854465202,
     "user": {
      "displayName": "Limpho Semakale",
      "userId": "11698043166095373885"
     },
     "user_tz": -120
    },
    "id": "compare_models",
    "outputId": "8b06bec9-2d18-4ab4-b6d3-35cd2d4b3312"
   },
   "outputs": [],
   "source": [
    "# Qualitative comparison: Base vs Fine-tuned\n",
    "test_questions = [\n",
    "    \"What are the symptoms of hypertension?\",\n",
    "    \"How is diabetes diagnosed?\",\n",
    "    \"What are the risk factors for heart disease?\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUALITATIVE COMPARISON: Base Model vs Fine-tuned Model\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    prompt = f\"\"\"Below is a medical question. Provide an accurate response.\n",
    "\n",
    "### Question:\n",
    "{question}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Test {i}: {question}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    # Base model\n",
    "    base_resp = generate_response(base_model, tokenizer, prompt, max_new_tokens=150)\n",
    "    if \"### Response:\" in base_resp:\n",
    "        base_resp = base_resp.split(\"### Response:\")[1].strip()\n",
    "\n",
    "    print(f\"\\n BASE MODEL:\")\n",
    "    print(base_resp[:300] + \"...\")\n",
    "\n",
    "    # Fine-tuned model\n",
    "    ft_resp = generate_response(model, tokenizer, prompt, max_new_tokens=150)\n",
    "    if \"### Response:\" in ft_resp:\n",
    "        ft_resp = ft_resp.split(\"### Response:\")[1].strip()\n",
    "\n",
    "    print(f\"\\n FINE-TUNED MODEL:\")\n",
    "    print(ft_resp[:300] + \"...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "experiment_tracking"
   },
   "source": [
    "# 8. Experiment Tracking\n",
    "\n",
    "Document your experiments for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1770854481271,
     "user": {
      "displayName": "Limpho Semakale",
      "userId": "11698043166095373885"
     },
     "user_tz": -120
    },
    "id": "create_experiment_table",
    "outputId": "acac0ad2-53d0-4df3-db58-8aff1fa4514e"
   },
   "outputs": [],
   "source": [
    "# Create experiment tracking table\n",
    "# Add your experiment results here\n",
    "\n",
    "experiments = [\n",
    "    {\n",
    "        'Experiment': 'Baseline (No Fine-tuning)',\n",
    "        'Learning_Rate': '-',\n",
    "        'Batch_Size': '-',\n",
    "        'Epochs': '-',\n",
    "        'LoRA_Rank': '-',\n",
    "        'BLEU': 0.0234,  # Example baseline\n",
    "        'ROUGE-L': 0.1245,\n",
    "        'Training_Time_min': '-',\n",
    "        'GPU_Memory_GB': '-'\n",
    "    },\n",
    "    {\n",
    "        'Experiment': 'Current Run',\n",
    "        'Learning_Rate': LEARNING_RATE,\n",
    "        'Batch_Size': BATCH_SIZE,\n",
    "        'Epochs': NUM_EPOCHS,\n",
    "        'LoRA_Rank': LORA_R,\n",
    "        'BLEU': float(avg_bleu),\n",
    "        'ROUGE-L': float(rouge_results['rougeL']),\n",
    "        'Training_Time_min': round(training_duration, 2),\n",
    "        'GPU_Memory_GB': round(torch.cuda.max_memory_allocated() / 1e9, 2) if torch.cuda.is_available() else '-'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df_experiments = pd.DataFrame(experiments)\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"EXPERIMENT TRACKING TABLE\")\n",
    "print(\"=\"*120)\n",
    "print(df_experiments.to_string(index=False))\n",
    "print(\"=\"*120)\n",
    "\n",
    "# Save to CSV\n",
    "df_experiments.to_csv(os.path.join(OUTPUT_DIR, 'experiment_results.csv'), index=False)\n",
    "print(\"\\n Experiment results saved to experiment_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "executionInfo": {
     "elapsed": 732,
     "status": "ok",
     "timestamp": 1770854499406,
     "user": {
      "displayName": "Limpho Semakale",
      "userId": "11698043166095373885"
     },
     "user_tz": -120
    },
    "id": "visualize_comparison",
    "outputId": "53fb4b15-f623-4f3c-81a8-7bee4b366edb"
   },
   "outputs": [],
   "source": [
    "# Visualize improvement\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# BLEU comparison\n",
    "experiments_to_plot = df_experiments[df_experiments['BLEU'] != '-']\n",
    "axes[0].bar(experiments_to_plot['Experiment'], experiments_to_plot['BLEU'], color=['lightcoral', 'lightgreen'])\n",
    "axes[0].set_ylabel('BLEU Score')\n",
    "axes[0].set_title('BLEU Score Comparison')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# ROUGE-L comparison\n",
    "axes[1].bar(experiments_to_plot['Experiment'], experiments_to_plot['ROUGE-L'], color=['lightcoral', 'lightgreen'])\n",
    "axes[1].set_ylabel('ROUGE-L Score')\n",
    "axes[1].set_title('ROUGE-L Score Comparison')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'model_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\" Visualization saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deployment_section"
   },
   "source": [
    "# 9. Deployment - Interactive Gradio Interface\n",
    "\n",
    "Create a web interface for interacting with your chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1770854508898,
     "user": {
      "displayName": "Limpho Semakale",
      "userId": "11698043166095373885"
     },
     "user_tz": -120
    },
    "id": "create_chatbot_function",
    "outputId": "0b5fdcf5-9245-4a22-bf14-b8996fd9579c"
   },
   "outputs": [],
   "source": [
    "# Create chatbot function\n",
    "def chat_with_healthcare_bot(message, history):\n",
    "    \"\"\"\n",
    "    Chat function for Gradio interface.\n",
    "    \"\"\"\n",
    "    # Check for emergency keywords\n",
    "    emergency_keywords = ['emergency', 'urgent', 'dying', 'suicide', 'chest pain', 'heart attack']\n",
    "    if any(keyword in message.lower() for keyword in emergency_keywords):\n",
    "        return (\"ðŸš¨ **This appears to be a medical emergency.** \"\n",
    "                \"Please call emergency services immediately (911 in the US) \"\n",
    "                \"or go to the nearest emergency room.\")\n",
    "\n",
    "    # Create prompt\n",
    "    prompt = f\"\"\"Below is a medical question. Provide an accurate, helpful, and professional response.\n",
    "\n",
    "### Question:\n",
    "{message}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "    # Generate response\n",
    "    response = generate_response(model, tokenizer, prompt, max_new_tokens=256)\n",
    "\n",
    "    # Extract response part\n",
    "    if \"### Response:\" in response:\n",
    "        response = response.split(\"### Response:\")[1].strip()\n",
    "\n",
    "    return response\n",
    "\n",
    "print(\" Chatbot function created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 801
    },
    "executionInfo": {
     "elapsed": 1052259,
     "status": "ok",
     "timestamp": 1770855564791,
     "user": {
      "displayName": "Limpho Semakale",
      "userId": "11698043166095373885"
     },
     "user_tz": -120
    },
    "id": "launch_gradio",
    "outputId": "edf4daa5-8a6c-4495-9a25-223ab6453a17"
   },
   "outputs": [],
   "source": [
    "# Create and launch Gradio interface\n",
    "disclaimer = \"\"\" **MEDICAL DISCLAIMER**: This chatbot is for educational and informational purposes only.\n",
    "It is NOT a substitute for professional medical advice, diagnosis, or treatment.\n",
    "Always seek the advice of qualified health providers with any questions regarding a medical condition.\"\"\"\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chat_with_healthcare_bot,\n",
    "    title=\" Healthcare Assistant Chatbot\",\n",
    "    description=\"Ask me questions about health conditions, symptoms, and medical information.\\n\\n\" + disclaimer,\n",
    "    examples=[\n",
    "        \"What are the symptoms of diabetes?\",\n",
    "        \"How can I lower my blood pressure naturally?\",\n",
    "        \"What are the risk factors for heart disease?\",\n",
    "        \"Explain the difference between Type 1 and Type 2 diabetes.\",\n",
    "        \"What should I do if I have a fever?\",\n",
    "        \"What are common side effects of antibiotics?\"\n",
    "    ],\n",
    "    theme=\"soft\"\n",
    ")\n",
    "\n",
    "# Launch interface\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LAUNCHING HEALTHCARE CHATBOT INTERFACE\")\n",
    "print(\"=\"*80)\n",
    "print(\"The interface will open in a new window.\")\n",
    "print(\"You can share the public URL to allow others to test your chatbot!\\n\")\n",
    "\n",
    "demo.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18687,
     "status": "ok",
     "timestamp": 1770855633385,
     "user": {
      "displayName": "Limpho Semakale",
      "userId": "11698043166095373885"
     },
     "user_tz": -120
    },
    "id": "kkQuiQ3_NzjQ",
    "outputId": "b36b5db6-74a8-482b-a520-c54102932958"
   },
   "outputs": [],
   "source": [
    "!pip install nbstripout\n",
    "!nbstripout healthcare_chatbot_finetuning.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28508,
     "status": "ok",
     "timestamp": 1770855848684,
     "user": {
      "displayName": "Limpho Semakale",
      "userId": "11698043166095373885"
     },
     "user_tz": -120
    },
    "id": "5treS1I4O12m",
    "outputId": "13d66ec3-2101-4675-e16b-2b303de76810"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ymy57DzqP2Eg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
